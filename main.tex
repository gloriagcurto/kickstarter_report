%\documentclass[a4paper]{article}

\documentclass{article}
\usepackage[a4paper]{geometry}

\usepackage[utf8]{inputenc}
\usepackage{import}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float} 
\usepackage{xcolor}
%\usepackage{subfigure}
\usepackage{caption}
\usepackage{listings} % JSON listing
\usepackage{eurosym}
\usepackage{parskip} % Do not indent the beginning of each paragraph
\usepackage{comment}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage[labelformat=parens,labelsep=quad,skip=3pt]{caption}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=orange,      
    urlcolor=blue,
}
\urlstyle{same}
%\renewcommand{\sectionname}{Lecture} % Redefine "chapter" to a different text
\usepackage{alphalph}
\renewcommand*{\thesubfigure}{%
\alphalph{\value{subfigure}}%
}%

\newcommand{\ToDo}[1]{\textcolor{magenta}{\textbf{[ToDo]} \textbf{#1}}}
%\newcommand{\gloria}[1]{\textcolor{Orange}{[Gloria] {#1}}}


\title{Classification of Kickstarter's successful or failed projects based on data crawled by a scraper robot (Web Robots)}
\author{Gloria GONZÁLEZ CURTO}
\date{\today}

\begin{document}

\maketitle

\tableofcontents


\section{Summary}
\label{ref:sum}
The aim of this project is to classify and ultimately predict the success or failure of a kickstarter fund rising campaign based on data monthly crawled from the kickstarter.com web site by a scraper robot (\url{http://webrobots.io}).

\section{Problem definition and background}
Kickstarter is an American corporation with a public benefit status launched in 2009 in the United States. The company initiated its international expansion in 2012. The kickstarter platform is open to backers from anywhere in the world and to creators from many countries.

Kickstarter maintains a global crowdfunding platform focused on creativity. Project creators choose a deadline and a minimum funding goal. If the goal is not met by the deadline, no funds are collected and the project is considered failed. Project backers usually receive rewards in exchange for their pledges.
Once a project collects enough pledges to bypass the goal before a deadline stablished by the creator, its state changes to successful. Only at that moment pledges are collected from backers. From that point onward, there is no guarantee for project delivery. Kickstarter advises backers to use their own judgment on supporting a project. They also warn project leaders that they could be liable for legal damages from backers for failure to deliver on promises.
 
On one hand, from a backer point of view, it would be interesting to have an analytic model to help with the decision of whether to pledge for a project or not. On the other hand, from the project creators, it would be interesting to have a guide to help them optimize their chances of success.

\section{Data description and pre-processing}
\label{sec:data_desc}

Raw data were collected from \url{https://webrobots.io/kickstarter-datasets/} in CSV format.  The downloaded data correspond to a crawl executed on February 13 2020. Raw data are composed by 57 CSV files with 3500 to 4000 rows each. See the subsection \ref{subsec:EDA} to access the profile report of the raw data. The raw original variables are:
\begin{itemize}

    \item \textbf{backers\_count}: integer representing how many people supported the project (backers).
    \item \textbf{blurb}: textual description of the project (around 150-200 characters).
    \item \textbf{category}: JSON-encoded description on the kickstarter's fixed categories: id, name, slug, position, parent\_id, parent\_name, color, urls.

    \item \textbf{converted\_pledged\_amount}: pleged amount converted to USD.
    \item \textbf{country}: country initials (i.e: FR for France):
    \item \textbf{country\_displayable\_name}: country name.
    \item \textbf{created\_at}: date on Unix time format.
    \item \textbf{creator}: JSON-encoded description on project's creator. id, name, is\_registered, chosen\_currency, is\_superbacker, avatar, small, medium, url\_web, url\_api. 
    \item \textbf{currency}: currency acronym.
    \item \textbf{currency\_symbol}.
    \item \textbf{currency\_trailing\_code}: boolean variable.
    \item \textbf{current\_currency}: USD.
    \item \textbf{deadline}: crowdfunding deadline in Unix time established by the project's creator.
    \item \textbf{disable\_communication}: boolean variable.
    \item \textbf{friends}.
    \item \textbf{fx\_rate}: conversion rate.
    \item \textbf{goal}: fund rising goal in creator's fixed currency.
    \item \textbf{id}: project identifier.
    \item \textbf{is\_backing}.
    \item \textbf{is\_starrable}.
    \item \textbf{is\_starred}.
    \item \textbf{launched\_at}: launch date on Unix time format.
    \item \textbf{location}: JSON-encoded description of the project's creator location: id, name, slug, short\_name, displayable\_name, localized\_name, country, state, type, is\_root, expanded\_country, url\_web, url\_location, url\_api\_nearby\_projects.
    \item \textbf{name}: project's name.
    \item \textbf{permissions}.
    \item \textbf{photo}: JSON-encoded description on project's photos: key, full, ed, med, little, small, thumb, 1024x576, 1536x864.
    \item \textbf{pledged}: pledged amount in original country currency.
    \item \textbf{profile}: JSON-encoded description on project's profile: id, project\_id, state, state\_changed\_at, name, blurb, background\_color, text\_color, link\_background\_color, link\_text\_color, link\_text, link\_url, show\_feature\_image, background\_image\_opacity, should\_show\_feature\_image\_section, feature\_image\_attributes, image\_urls. 
    \item \textbf{slug}: small textual description with - as spacers.
    \item \textbf{source\_url}: url pointing to project's category site on kickstarter.
    \item \textbf{spotlight}: boolean representing if the project was featured on kickstarter web site.
    \item \textbf{staff\_pick}: boolean representing if the project was picked by kickstarter staff.
    \item \textbf{state}: project's state initialy composed by 5 categories (successful, failed, canceled, suspended and live.
    \item \textbf{state\_changed\_at}: date in Unix time format.
    \item \textbf{static\_usd\_rate}: static rate for currency conversion into USD.
    \item \textbf{urls}: JSON-encoded description containing urls: project, rewards.
    \item \textbf{usd\_pledged}.
    \item \textbf{usd\_type}: domestic.
    
\end{itemize} 

\subsection{Data pre-processing}
\label{subsec:data_prepro}
The following steps were followed to obtain a clean dataset for model training or EDA from raw data.
\begin{itemize}
    \item \textbf{Join CSV files}.
Raw data contain ambigous JSON-encoded fields that induce errors while parsing. To avoid those errors, ambigous JSON-encoded containing rows were filtered out while joining data into an unique file , see {\tt pre\_join.py}.

    \item \textbf{Deserialize JSON-encoded columns}.
Columns and rows with missing information were dropped ("friends", "is\_backing", "is\_starred", "permissions") from raw data before JSON deserialization.
JSON-encoded columns (\emph{'category', 'creator', 'location', 'photo', 'profile', 'urls'}) were isolated from the rest of raw data and deserialized using a custom made function (\emph{deserialize\_in\_batch}) by means of the json\_normalize method from the pandas package and loads from the json package. The implementation is available at {\tt pre\_decode\_JSON.py}).

    \item \textbf{Selection and pre-processing of deserialized variables}.
    The implementation of the following steps can be accessed at {\tt pre\_deserialized\_to\_one\_hot\_encoding.py}.
    \begin{itemize}
 
        \item Drop category\_slug because it contains partial redundant information with category\_name and category\_parent\_name.
        \item Fill category\_parent\_name missing values with their corresponding category\_name value.
        \item Exploration of location related variables: location\_localized\_name, location\_country, and location\_expanded\_country
	\begin{itemize}
            \item location\_localized\_name contains no missing values and 12977 unique values. Such a number of levels for a categorical variable might not be informative. Drop location\_localized\_name.
            \item location\_country and location\_expanded\_country are redundant. Drop location\_country because is less human readable.
	\end{itemize}
	\item Profile related variables. Codification of profile related variables into a binary code reflecting the involvement of the project creator in the generation of a profile.
	\begin{itemize}
            \item Re-codification of categorical binary variables:
	    profile\_state: "inactive"=0, "active"=1
            profile\_show\_feature\_image: False=0, True=1
            profile\_should\_show\_feature\_image\_section: False=0, True=1

            \item The rest of the profile related columns are going to be coded in a binary choice variable (missing value = 0= , variable contains a project creator provided value =1). The following columns were processed: 'profile\_name', 'profile\_blurb', 'profile\_background\_color', 'profile\_text\_color', 'profile\_link\_background\_color', 'profile\_link\_text\_color', 'profile\_link\_text', 'profile\_link\_url'.
        \end{itemize}
	\item One hot encoding of categorical variables while keeping the original columns for EDA.
        The variables 'category\_name', 'category\_parent\_name', 'location\_expanded\_country' were codified using pd.get\_dummies and the argument drop\_first was set to True.
   \end{itemize}
   \item \textbf{Change date format and compute date derived variables}.
    The implementation is available at {\tt pre\_date\_format\_and\_derived\_variables.py}.
    
    First, date and time coding variables ('created\_at', 'deadline', 'launched\_at', 'state\_changed\_at') in the raw data are in Unix time format and need to be transformed into datetime. The following variables where computed from the original ones: weekday columns for each date variable, month columns for each date variable, year columns for each date variable.
    
    'initial\_found\_rising\_duration' was computed as the difference in days between 'deadline' and 'launched\_at'.
    
    'found\_rising\_duration' was computed as the difference in days between 'state\_changed\_at' and 'launched\_at'.
    
    'project\_set\_up\_duration' was computed as the difference in days between 'launched\_at' and 'created\_at'.
    
    Date variables were prunned before model training to remove the 'state\_changed\_at' derived ones.
    
    \item \textbf{Currency derived variables pre-processing}.
    
    The implementation is available at {\tt pre\_currency\_and\_other\_variables.py}.
    
    The following original variables were dropped due to redundancies or because they are not informative for the model: 'current\_currency', 'currency\_trailing\_code', 'fx\_rate', 'converted\_pledged\_amount', 'pledged', 'slug', 'usd\_type'.
    
    The variable 'usd\_goal' was computed by multiplication of the variables 'goal' 'and 'static\_usd\_rate' to able to compare the pledge goals for all the projects. 'goal' and 'static\_usd\_rate' were dropped after the generation of 'usd\_goal'.
    
    The variable 'is\_starrable' was also dropped, because of lack of relevance.
    
    The variables 'disable\_communication', 'spotlight' and 'staff\_pick' were recodified by replacing False=0 and True=1.
    
    The variable currency was dummified using get\_dummies (the original varaible was kept for visualization purpose).

    \item \textbf{State pre-processing (target variable)}.
    
    The implementation is available at {\tt pre\_currency\_and\_other\_variables.py}.
    
    State is the target variable for classification. The raw variable is a categorical variable with 5 levels('successful', 'failed', 'live', 'canceled', 'suspended') and  no missing values.
    
    The variable 'state\_group' was generated by grouping the canceled and suspended projects into 'failed'. It contains then 3 distinct levels ('successful', 'failed' and 'live').
    
    The variable 'state\_code' was generated by dummification from 'state\_grouped' (failed = 0, successful=1, live=2). There are 114940 successful projects, 84311 failed projects and 6651 live projects.

   \item \textbf{Join the JSON decoded variables to the rest of the data}.
   
   The implementation is available at {\tt pre\_join\_and\_drop\_live\_state.py}.
   
    \item \textbf{Drop rows corresponding to projects on a "live" state}.
    
    The implementation is available at {\tt pre\_join\_and\_drop\_live\_state.py}.
    
    \item \textbf{Evaluate and drop duplicate rows}.
    
    The implementation is available at {\tt pre\_duplicates.py}.
    21392 duplicate rows were detected and removed before further processing using pandas duplicated and drop\_duplicates functions.
    
    \item \textbf{Drop variables carrying low information}.
    
    Data dimensions at this processing stage was 177859 rows and 428 columns. In the following scripts I will evaluate the informative potential of different subsets of variables and drop the less informative ones in order to reduce the number of binary encoded columns and reduce the overfitting potential.
    
    \begin{itemize}
	\item Generation of the variable 'profile'.
	
        The implementation is available at {\tt pre\_profile\_pruning.py}.
	
	'profile' was generated by addition of the following columns: 'profile\_ state', 'profile\_ name', 'profile\_ blurb', 'profile\_ background\_ color']+ df['profile\_ text\_ color', 'profile\_ link\_ background\_ color', 'profile\_ link\_ text\_ color', 'profile\_ link\_ text', 'profile\_ link\_ url', 'profile\_ show\_ feature\_ image', 'profile\_ should\_ show\_ feature\_ image\_ section'. It represents an score accounting for profile completeness. The variables were dropped after the score was computed.
        
	The number of columns was reduced to 418.
	
	\item Category name pruning.
	
	The implementation is available at {\tt pre\_category\_name\_pruning.py }.
	
	'category\_name\_ori' encodes 159 subcategories of the 15 parent categories for kickstarter's projects. In order to evaluate the information content of each of these subcategory levels, plots and percentages of successful projects by secondary category were plotted to decide which one hot encoded columns under category\_name to drop. Only categories with a percentage of successful projects of at least 55\% (project success rate for the data is 53\%) that were informative within their parent categories were kept.
	
	After verification of plots and percentages, all comic, theater, dance secondary categories contain more successful tha failed projects, see Figure~\ref{fig:countCat}. As they are not more informative than their parent categories, those secondary categories were removed. 
	
\begin{figure}
  \begin{subfigure}[a]{0.8\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/variable_pruning_EDA/cat_name/nprojects_Comics_state.pdf}
    \caption{Count plot of successful and failed projects and subcategories of Comics principal category}
    \label{fig:countCatA}
  \end{subfigure}
  \begin{subfigure}[b]{0.8\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/variable_pruning_EDA/cat_name/nprojects_Dance_state.pdf}
    \caption{Count plot of successful and failed projects and subcategories of Dance principal category}
    \label{fig:countCatB}
  \end{subfigure}
  \begin{subfigure}[c]{0.8\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/variable_pruning_EDA/cat_name/nprojects_Theater_state.pdf}
    \caption{Count plot of successful and failed projects and subcategories of Dance principal category}
    \label{fig:countCatC}
  \end{subfigure}
\caption{Count plots of successful and failed projects within categories.}
\label{fig:countCat}
\end{figure}

	The following subcategories were kept: 'Anthologies', 'Letterpress', "Children's Books", 'Art Books', 'Publishing', 'Literary Spaces', 'Fiction', 'Nonfiction', 'Playing Cards', 'Video Games', 'Tabletop Games', 'Games', 'Puzzles', 'Journalism', 'Public Art', 'Illustration', 'Painting', 'Art', 'Social Practice', 'Shorts', 'Narrative Film', 'Documentary', 'Webseries', 'Film \& Video', 'Crafts', 'Photography', 'Product Design', 'Typography', 'Design', 'Classical Music', 'Country \& Folk', 'Indie Rock', 'Pop', 'Music', 'Jazz', 'Comedy', 'Rock', 'Chiptune', 'Apparel', 'Accessories', 'Fashion', 'Gadgets', 'Hardware', 'Technology'.
	
	The number of columns was reduced to 304.
	
        \item Country pruning.
        The implementation is available at {\tt pre\_country\_pruning.py}.  
	
	Projects in the dataset come from 198 countries. In order to reduce the number of variables, I performed count plots and percentages of successful projects by country. Only countries with more than 55\% of successful projects and more than 50 projects were kept (United Kingdom, Hong Kong, Japan, Singapore, China, Poland, Israel, Taiwan, Czech Republic, Greece, Indonesia, Argentina, Kenya, Iceland, Ghana, Portugal, Slovenia, Finland).
	
	The number of columns was reduced to 125.
	
	\item  Currency pruning.
	The implementation is available at {\tt pre\_currency\_pruning.py }.
	
	Currency encodes 14 different levels. Count plots and percentages of succesfull projects were computed for tha variable 'currency\_orig'. Currencies with more than 55\% of successful projects were retained (GBP, HKD, SGD, JPY) and compared to the retained countries. All currency levels were dropped because the selected currencies correspond to countries that were already selected and the information was redundant.
	
	The number of columns was reduced to 112.
        
    \end{itemize}
    \item \textbf{Drop rows with text in other languages than English}.
    The implementation is available at {\tt pre\_words.py}.
    
    The variables blurb and name contain brief descriptions of the project. In order to obtain valuable information for the model, these text variables are going to be processed to obtain keywords and compute a score due to their presence in the text. Project\'s texts are writen in several languages. Several attempts to translate all text into English were performed using langdetect package. Unfortunately, the quantity of characters to translate was bigger than the established API limits. The estimation of the fees for the translation using the google translate API was 1200~\euro.
    
    English rows were detected and isolated using a custom made function. Briefly, for each row (project), blurb and name texts were tokenized and then compared to an extense list of 370101 English words \url{(https://github.com/dwyl/english-words/blob/master/words_dictionary.json)}. An score was computed considering tokens with more than 3 characters and the lenght of the text. Rows with an score lower than 0.8 were considered not written in English and discarded (55269 rows).
    
    Data dimension after filtering was 122590 rows and 112 columns.
    
    To be able to extract category keywords associated to successful or failed projects, data were splitted in training and test sets at this point.
    
    \item \textbf{Split train and test}.
    
    The implementation is available at {\tt pre\_split\_train\_test.py }.
    
    Before splitting the data in train and test sets, I visualized the projects by year ("state\_changed\_at") and state (see Figure \ref{fig:time}).
    
\begin{figure}
  \begin{subfigure}[a]{0.9\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/nprojects_date_state_change.pdf}
    \caption{Total, successful and failed projects in time}
    \label{fig:linetime}
  \end{subfigure}
  \begin{subfigure}[b]{0.9\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/variable_pruning_EDA/year/nprojects_year_state.pdf}
    \caption{Count plot of successful and failed projects by year}
    \label{fig:countYear}
    \end{subfigure}
\caption{Temporal dimension of the target variable}
\label{fig:time}
\end{figure}

    The implementation is available at {\tt pre\_year\_EDA.py}  and
    
    {\tt pre\_data\_prep\_temporal\_and\_classification.py}.
    
    The target variable (state) shows no clear correlation with time, as we can observe in Figure \ref{fig:time}. Moreover, our data are not evenly spaced in time. Therefore, after close examination of our data, I decided to implement a random split of train and test sets using the train\_test\_split functiikit learn model\_selection package, a train size of 73 \% a 37 as random seed. Before spliting, 15 columns were dropped, containing information not available prior to the project's state change. Dimensions of training set are 89490 rows and 97 columns, and dimensions of test set are 33100 rows and 97 columns. 
	
    \item \textbf{Generation of the frequency score feature}.
    
    The implementation is available at {\tt pre\_text\_mining.py}.
    
    Title and descriptions of the project were processed to obtain an score based on word frequencies in succesful and failed projects by principal category ('category\_parent\_name', 15 categories). Word  frequencies were computed using both 'blurb' and 'name' variables on the training set. Punctuation signs were replaced by blank spaces, words were tokenized (word\_tokenize from nltk package), stop words were filtered (stopwords from nltk), and stemmed (PorterStemmer from nltk). Word frequency was computed using FreqDist (nltk package). The frequency score was computed for every project by adding the successful project frequency of every word present in its 'blurb' and 'name' variables and substracting the failed project frequencies. The score was normalized by text lenght.  

\end{itemize}

\subsection{Exploratory data analysis}
\label{subsec:EDA}
\begin{itemize}

\item \textbf{Profile reports} generated multiple times during data pre-processing using the pandas profile package (\url{https://pypi.org/project/pandas-profiling/}) to help with data visualization. Raw, train and test data htlm profiling reports can be found at \href{http://gloriagcurto.info/EDA_profiles/}{my professional website}.

The implementation is available at {\tt EDA\_profiling.py} and {\tt EDA\_tt\_profile.py}.

Pandas profiling generates an statistical description of the data and each variable, computes interaction and correlation plots and warns about potential sources of problems such as high correlation, duplicates, high cardinality for categorical variables and skewed or zero containing variables.
These reports (available at \url{http://gloriagcurto.info/EDA_profiles}) were extremely useful to quickly identify duplicated rows, variables with high numbers of missing values and also non numerical variables present in the train and test sets that are incompatible with XGBoost modeling.

\item \textbf{Word clouds} were generated from the train and test sets by principal category and successful or failed projects using WordCloud.

The implementation is available at {\tt pre\_text\_mining.py}.

Word clouds were representative of each category and sometimes were helpful to identify subcategories with high rates of failed projects (such as food trucks within Food, see Figure \ref{fig:Food_f}). The majority of the most frequent 200 words were shared by successful and failed projects within categories but they also always present especific more frequent words, see Figure \ref{fig:wordclouds} and \ref{fig:wordclouds_test}).
If we take as an example the Comic word clouds (Figure \ref{fig:Comic_s}, \ref{fig:Comic_f}), we can see that both successful and failed projects contain graphic novel, adventure and comic with aproximatively the same frequency, but horror, fantasy and manga, are overrepresented in the failed projects. At the same time, anthologies, sci fi and girl terms are overrepresented in the succesful projects, suggesting possible differences in the succesful rate of different comic genres.
Other words were consistenly found associated to successful projects across categories. This is the case, for example, of enamel pin and hard enamel that were associated to succesful projects from the crafts (Figure \ref{fig:Crafts_s}) and fashion categories (Figure \ref{fig:Fashion_s}), and absent from the 200 most frequent words ( see Figures \ref{fig:Crafts_f} and \ref{fig:Fashion_f}).

For clarity purpose, test set word clouds can be found at the end of the document, see Figure \ref{fig:wordclouds_test}.

Word clouds were useful to understand project dinamics within categories and to evaluate the pertinency of the frecuency score feature. As we can identify trends in the word clouds associated to the target variable, the frequency score could potentialy help increase the evaluation metrics of our model.

\begin{figure}
  \begin{subfigure}[a]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Art_successful.pdf}
    \caption{Successful Art}
    \label{fig:Art_s}
  \end{subfigure}
  \begin{subfigure}[a']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Art_failed.pdf}
    \caption{Failed Art}
    \label{fig:Art_f}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Comics_successful.pdf}
    \caption{Successful Comics}
    \label{fig:Comic_s}
  \end{subfigure} 
  \begin{subfigure}[b']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Comics_failed.pdf}
    \caption{Failed Comics}
    \label{fig:Comic_f}
  \end{subfigure}
  \begin{subfigure}[c]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Crafts_successful.pdf}
    \caption{Successful Crafts}
    \label{fig:Crafts_s}
  \end{subfigure}
    \begin{subfigure}[c']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Crafts_failed.pdf}
    \caption{Failed Crafts}
    \label{fig:Crafts_f}
  \end{subfigure}
    \begin{subfigure}[d]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Dance_successful.pdf}
    \caption{Successful Dance}
    \label{fig:Dance_s}
  \end{subfigure}
    \begin{subfigure}[d']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Dance_failed.pdf}
    \caption{Failed Dance}
    \label{fig:Dance_f}
   \end{subfigure}
    \begin{subfigure}[e]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Design_successful.pdf}
    \caption{Successful Design}
    \label{fig:Design_s}
  \end{subfigure} 
  \begin{subfigure}[e']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Design_failed.pdf}
    \caption{Failed Design}
    \label{fig:Design_f}
  \end{subfigure} 
    \begin{subfigure}[f]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Fashion_successful.pdf}
    \caption{Successful Fashion}
    \label{fig:Fashion_s}
  \end{subfigure} 
  \begin{subfigure}[f']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Fashion_failed.pdf}
    \caption{Failed Fashion}
    \label{fig:Fashion_f}
  \end{subfigure}
    \begin{subfigure}[g]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Film_Video_successful.pdf}
    \caption{Successful Film \& Video}
    \label{fig:Film_Video_s}
  \end{subfigure} 
  \begin{subfigure}[g']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Film_Video_failed.pdf}
    \caption{Failed Film \& Video}
    \label{fig:Film_Video_f}
  \end{subfigure}
    \begin{subfigure}[h]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Food_successful.pdf}
    \caption{Successful Food}
    \label{fig:Food_s}
  \end{subfigure} 
  \begin{subfigure}[h']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Food_failed.pdf}
    \caption{Failed Food}
    \label{fig:Food_f}
  \end{subfigure}
    \begin{subfigure}[i]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Games_successful.pdf}
    \caption{Successful Games}
    \label{fig:Games_s}
  \end{subfigure} 
  \begin{subfigure}[i']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Games_failed.pdf}
    \caption{Failed Games}
    \label{fig:Games_f}
  \end{subfigure}
  \begin{subfigure}[j]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Journalism_successful.pdf}
    \caption{Successful Journalism}
    \label{fig:Journalism_s}
  \end{subfigure} 
  \begin{subfigure}[j']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Journalism_failed.pdf}
    \caption{Failed Journalism}
    \label{fig:Journalism_f}
  \end{subfigure}
  \begin{subfigure}[k]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Music_successful.pdf}
    \caption{Successful Music}
    \label{fig:Music_s}
  \end{subfigure} 
  \begin{subfigure}[k']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Music_failed.pdf}
    \caption{Failed Music}
    \label{fig:Music_f}
  \end{subfigure}
  \begin{subfigure}[l]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Photography_successful.pdf}
    \caption{Successful Photography}
    \label{fig:Photography_s}
  \end{subfigure} 
  \begin{subfigure}[l']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Photography_failed.pdf}
    \caption{Failed Photography}
    \label{fig:Photography_f}
  \end{subfigure}
    \begin{subfigure}[m]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Publishing_successful.pdf}
    \caption{Successful Publishing}
    \label{fig:Publishing_s}
  \end{subfigure} 
  \begin{subfigure}[m']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Publishing_failed.pdf}
    \caption{Failed Publishing}
    \label{fig:Publishing_f}
  \end{subfigure}
    \begin{subfigure}[n]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Technology_successful.pdf}
    \caption{Successful Technology}
    \label{fig:Technology_s}
  \end{subfigure} 
  \begin{subfigure}[n']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Technology_failed.pdf}
    \caption{Failed Technology}
    \label{fig:Technology_f}
  \end{subfigure}
    \begin{subfigure}[o]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Theater_successful.pdf}
    \caption{Successful Theater}
    \label{fig:Theater_s}
  \end{subfigure} 
  \begin{subfigure}[o']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_train_Theater_failed.pdf}
    \caption{Failed Theater}
    \label{fig:Theater_f}
  \end{subfigure}

\caption{Wordclouds of successful and failed projects by principal category in the training set.}
\label{fig:wordclouds}
\end{figure}

The rest of data visualization was performed as a guide during data pre-processing (see Sections \ref{subsec:data_prepro}, \ref{sec:interp} and Figures \ref{fig:countCat}, \ref{fig:time}, \ref{fig:goal},  \ref{fig:profile}, \ref{fig:freq}).

\end{itemize}


\section{Data modeling}
\label{sec:model}
\begin{itemize}
\item \textbf{Data processing prior to XGBoost training}
The implementation is available at {\tt pre\_data\_model\_train.py} and {\tt pre\_data\_model\_test.py}.
First, the target variable in numeric format ('state\_code') was isolated from the features and written to a file.
Second, non numeric features were identified thanks to the pandas-profiling report and removed. Features were written to a file.

\item \textbf{Methodological choices}
The following packages were used for data modeling, evaluation and interpretation:
\begin{itemize}
	\item xgboost.
	Extreme Gradient Boosting classifier~\cite{DBLP:journals/corr/ChenG16} was chosen due to its performance, escalabilty and built-in model interpretation capabilities.
The tree based booster was prefered because of the intuitive interpretation of tree models.
	\item Scikit learn matrics: classification\_report, confusion\_matrix~\cite{pedregosa2011scikit}.
	\item bayesian-optimization~\cite{nogueiraBayOpt}
	Bayesian optimization with cross validation was prefered as hyperparameter tuning method over a grid or a random search because it has been shown to obtain better results in fewer evaluations~\cite{snoek2012practical, NIPS2011_4443}.

	\item SHAP
	SHAP ~\cite{DBLP:journals/corr/LundbergL17, lundberg2020local2global} was prefered over other model interpretation methods due to its mathematical strenght.
\end{itemize}

\item \textbf{Model training, first iteration}

The implementation is available at {\tt train\_first\_model\_wo\_ts.py}.

As a first attempt to model the data, a XGboost tree bosster classifier was initialized with default parameter and fitted to the training data. Evaluation metrics for predictions using both the training and test set were consistently found to show a score of 1, see tables \ref{table:model1_tr_cr}, \ref{table:model1_te_cr}.

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &  recall &  f1-score &  support \\
\midrule
0            &        1.0 &     1.0 &       1.0 &  41522.0 \\
1            &        1.0 &     1.0 &       1.0 &  47968.0 \\
accuracy     &        1.0 &     1.0 &       1.0 &      1.0 \\
macro avg    &        1.0 &     1.0 &       1.0 &  89490.0 \\
weighted avg &        1.0 &     1.0 &       1.0 &  89490.0 \\
\bottomrule
\end{tabular}
\caption{Classification report for training set}
\label{table:model1_tr_cr}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &  recall &  f1-score &  support \\
\midrule
0            &        1.0 &     1.0 &       1.0 &  15423.0 \\
1            &        1.0 &     1.0 &       1.0 &  17677.0 \\
accuracy     &        1.0 &     1.0 &       1.0 &      1.0 \\
macro avg    &        1.0 &     1.0 &       1.0 &  33100.0 \\
weighted avg &        1.0 &     1.0 &       1.0 &  33100.0 \\
\bottomrule
\end{tabular}
\caption{Classification report for test set}
\label{table:model1_te_cr}
\end{table}

In order to diagnose the reason behind such a perfect metrics, training set label randomization was performed using shuffle from scikit learn utils package. A new model was fitted with the shuffled labels and predictions for both the training and test sets were evaluated. As shown in tables \ref{table:model1_tr_sh_cr}, \ref{table:model1_te_sh_cr} and in their corresponding confusion matrices  \ref{table:model1_tr_sh_cm}, \ref{table:model1_te_sh_cm}, predictions were randomized after shuffling the labels, as proben by an accuracy of  50\% in predictions for the test set. These results suggest that the perfect scores are not due to overfitting.
 
\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.726857 &  0.325442 &  0.449587 &  41522.000000 \\
1            &   0.604945 &  0.894138 &  0.721647 &  47968.000000 \\
accuracy     &   0.630272 &  0.630272 &  0.630272 &      0.630272 \\
macro avg    &   0.665901 &  0.609790 &  0.585617 &  89490.000000 \\
weighted avg &   0.661510 &  0.630272 &  0.595415 &  89490.000000 \\
\bottomrule
\end{tabular}
\caption{Classification report for predictions of the train set using the model trained with shuffled labels}
\label{table:model1_tr_sh_cr}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrr}
\toprule
{} &      0 &      1 \\
\midrule
0 &  13513 &  28009 \\
1 &   5078 &  42890 \\
\bottomrule
\end{tabular}

\caption{Confusion matrix for predictions of the train set using the model trained with shuffled labels}
\label{table:model1_tr_sh_cm}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.419684 &  0.184141 &  0.255971 &  15423.000000 \\
1            &   0.522159 &  0.777847 &  0.624858 &  17677.000000 \\
accuracy     &   0.501208 &  0.501208 &  0.501208 &      0.501208 \\
macro avg    &   0.470921 &  0.480994 &  0.440415 &  33100.000000 \\
weighted avg &   0.474410 &  0.501208 &  0.452975 &  33100.000000 \\
\bottomrule
\end{tabular}
\caption{Classification report for predictions of the test set using the model trained with shuffled labels}
\label{table:model1_te_sh_cr}
\end{table}
 
\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.419684 &  0.184141 &  0.255971 &  15423.000000 \\
1            &   0.522159 &  0.777847 &  0.624858 &  17677.000000 \\
accuracy     &   0.501208 &  0.501208 &  0.501208 &      0.501208 \\
macro avg    &   0.470921 &  0.480994 &  0.440415 &  33100.000000 \\
weighted avg &   0.474410 &  0.501208 &  0.452975 &  33100.000000 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for predictions of the test set using the model trained with shuffled labels}
\label{table:model1_te_sh_cm}
\end{table}

Tree and importance plots for model interpretation implemented in the XGBoost package solved the mistery, see Figure \ref{fig:xgb_int_spot}. Interpretation plots showed that the decision tree had a unique branch and the predictions were solely based in the value of the feature 'spotlight', that was identified as highly correlated with the target variable by the \href{http://gloriagcurto.info/EDA_profiles}{pandas-profiling reports}.

After a search in the kickstater's website, I found a more exact difinition for 'spotlight' that the one provided with the data. Spotlight is a place for the projects on Kickstarter were the creators can communicate their project's progress, after they’ve been successfully funded.

I removed 'spotlight' from the features files and proceed to train a new model.

\begin{figure}
  \begin{subfigure}[a]{0.5\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/xgb_plots/xgbcl1_tree.pdf}
    \caption{Decision tree plot}
    \label{fig:tree_spot}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/xgb_plots/xgbcl1_importance_cover_plot.pdf}
    \caption{Importance cover plot}
    \label{fig:cover_spot}
  \end{subfigure}
  \begin{subfigure}[c]{0.5\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/xgb_plots/xgbcl1_importance_gain_plot.pdf}
    \caption{Importance gain plot}
    \label{fig:gain_spot}
  \end{subfigure}
  \begin{subfigure}[d]{0.5\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/xgb_plots/xgbcl1_importance_weight_plot.pdf}
    \caption{Importance weight plot}
    \label{fig:weight_spot}
  \end{subfigure}
\caption{XGBoost built-in model interpretation plots.}
\label{fig:xgb_int_spot}
\end{figure}

\item \textbf{Model training, second iteration}

The implementation is available at {\tt train\_wo\_spotlight.py}.

A bayesian Optimization function for xgboost was defined with the following parameters:
\begin{itemize}
	\item 'objective': 'binary:logistic'. Represents the learning task and the corresponding learning objective for a binary classification.
	\item 'max\_depth': int(max\_depth). Maximum depth of a tree. Higher values make the model more complex and more likely to overfit. Range: $(0, \infty)$.
	\item 'gamma': gamma. Minimum loss reduction required to partition a leaf node of the tree. Higher values make the algorithm more conservative. Range: $(0, \infty)$.
	\item'learning\_rate':learning\_rate. Step size shrinkage used in boosting iterations. Higher values make the process more conservative. Range: (0,1).
	\item 'subsample': subsample. Subsample ratio of the training instancesprior to growing trees. Subsampling will occur once in every boosting iteration and prevents overfitting. Range: (0,1).
	\item 'eval\_metric': 'auc'. AUC measures the quality of the model's predictions irrespective of what classification threshold is chosen and their absolute values. For our classification task, we don't need well calibrated outputs and the cost of classification errors in our case is not critical. Moreover, a paralel hyperparameter optimization was evaluated using 'error' as metric with similar results. Range: (0,1).

\end{itemize}

Cross validation was performed in 5 folds and 500 iterations, with and early stop at 100 rounds.
Hyperparameter search was evaluated using "test-auc-mean" as metric.

The bayesian hyperparameter space was designed in a conservative way, to avoid overfitting of the data.
\begin{itemize}
	\item 'max\_depth': (3, 8).  Default value is 6. 
	\item 'gamma': (0, 5). Default value is 0.
	\item 'learning\_rate':(0, 1). Default value is 0.3.
	\item 'subsample':(0, 1). Default value is 1.
\end{itemize}

Bayesian optimization was performed for 5 iterations with 8 steps of random exploration and an acquisition function of expected improvement ('ie'). Best parameters were optimized to \{'gamma': 3.262977074427203, 'learning\_rate': 0.08113939174319618, 'max\_depth': 7.84163489438336, 'subsample': 0.9915277034216099\}.

The following tables contain evaluation metrics obtained by training the model with the best hyperparameters defined by bayesian optimization:

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.833378 &  0.896802 &  0.863927 &  41522.000000 \\
1            &   0.904370 &  0.844792 &  0.873566 &  47968.000000 \\
accuracy     &   0.868924 &  0.868924 &  0.868924 &      0.868924 \\
macro avg    &   0.868874 &  0.870797 &  0.868747 &  89490.000000 \\
weighted avg &   0.871431 &  0.868924 &  0.869094 &  89490.000000 \\
\bottomrule
\end{tabular}
\caption{Classification report for predictions of the train set using bayesian optimization hyperparameters}
\label{table:model_tr_bo_cr}
\end{table}

\begin{table}[h!]
\centering

\begin{tabular}{lrr}
\toprule
{} &      0 &      1 \\
\midrule
0 &  37237 &   4285 \\
1 &   7445 &  40523 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for predictions of the train set using bayesian optimization hyperparameters}
\label{table:model_tr_bo_cm}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &      support \\
\midrule
0            &   0.829042 &  0.885107 &  0.856157 &  15423.00000 \\
1            &   0.893471 &  0.840754 &  0.866311 &  17677.00000 \\
accuracy     &   0.861420 &  0.861420 &  0.861420 &      0.86142 \\
macro avg    &   0.861256 &  0.862930 &  0.861234 &  33100.00000 \\
weighted avg &   0.863450 &  0.861420 &  0.861580 &  33100.00000 \\
\bottomrule
\end{tabular}
\caption{Classification report for predictions of the test set using bayesian optimization hyperparameters}
\label{table:model_te_bo_cr}
\end{table}
 
\begin{table}[h!]
\centering
\begin{tabular}{lrr}
\toprule
{} &      0 &      1 \\
\midrule
0 &  13651 &   1772 \\
1 &   2815 &  14862 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for predictions of the test set using bayesian optimization hyperparameters}
\label{table:model_te_bo_cm}
\end{table}

If we compare these classification reports, in tables \ref{table:model_tr_bo_cr}, \ref{table:model_te_bo_cr} to the ones of a XGBoost classifier with default parameters, in tables, \ref{table:model_tr_cr}, \ref{table:model_te_cr}, we can observe similar metrics. However, the confusion matrices for the default hyperparameters model, see tables \ref{table:model_tr_cm}, \ref{table:model_te_cm}, were slighly better than for the bayesian optimization model, see tables \ref{table:model_tr_bo_cm}, \ref{table:model_te_bo_cm}. Therefore, I decided to save the default parameters model to a file for further exploitation.  The following step was model interpretation.

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.835575 &  0.906652 &  0.869664 &  41522.000000 \\
1            &   0.912773 &  0.845564 &  0.877884 &  47968.000000 \\
accuracy     &   0.873908 &  0.873908 &  0.873908 &      0.873908 \\
macro avg    &   0.874174 &  0.876108 &  0.873774 &  89490.000000 \\
weighted avg &   0.876955 &  0.873908 &  0.874070 &  89490.000000 \\
\bottomrule
\end{tabular}

\caption{Classification report for predictions of the train set using default hyperparameters}
\label{table:model_tr_cr}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrr}
\toprule
{} &      0 &      1 \\
\midrule
0 &  37646 &   3876 \\
1 &   7408 &  40560 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for predictions of the train set using default hyperparameters}
\label{table:model_tr_cm}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.828747 &  0.892369 &  0.859382 &  15423.000000 \\
1            &   0.899351 &  0.839113 &  0.868188 &  17677.000000 \\
accuracy     &   0.863927 &  0.863927 &  0.863927 &      0.863927 \\
macro avg    &   0.864049 &  0.865741 &  0.863785 &  33100.000000 \\
weighted avg &   0.866453 &  0.863927 &  0.864085 &  33100.000000 \\
\bottomrule
\end{tabular}
\caption{Classification report for predictions of the test set using default hyperparameters}
\label{table:model_te_cr}
\end{table}
 
\begin{table}[h!]
\centering
\begin{tabular}{lrr}
\toprule
{} &      0 &      1 \\
\midrule
0 &  13763 &   1660 \\
1 &   2844 &  14833 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for predictions of the test set using default hyperparameters}
\label{table:model_te_cm}
\end{table}

\end{itemize}

\section{Model interpretation}
\label{sec:interp}

\begin{itemize}
\item \textbf{XGBoost built-in model interpretation}

The implementation is available at {\tt train\_wo\_spotlight.py}.

\begin{figure}
  \begin{subfigure}{1\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/xgb_plots/xgbcl_wo_spot_tree}
  \end{subfigure}
\caption{Decision tree plot}
\label{fig:tree}
\end{figure}

\begin{figure}

  \begin{subfigure}[a]{0.5\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/xgb_plots/xgbcl_wo_spot_importance_cover_plot.pdf}
    \caption{Importance cover plot}
    \label{fig:cover}
  \end{subfigure}
 
  \begin{subfigure}[b]{0.5\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/xgb_plots/xgbcl_wo_spot_importance_weight_plot.pdf}
    \caption{Importance weight plot}
    \label{fig:weight}
  \end{subfigure}
  
   \begin{subfigure}[c]{0.5\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/xgb_plots/xgbcl_wo_spot_importance_gain_plot.pdf}
    \caption{Importance gain plot}
    \label{fig:gain}
  \end{subfigure}
\caption{XGBoost built-in model interpretation plots.}
\label{fig:xgb_int}
\end{figure}

According to the model's decision tree in Figure \ref{fig:tree}, the first variable that is considered for classification is 'profile', the second is 'usd\_goal'. If we look at the importance plots (Figure \ref{fig:xgb_int}), we can see that even though 'profile' is always important, its order of appearance is not conserved. Differences are due to different methods to compute the importance:
\begin{itemize}

\item ”weight”: number of times a feature appears in a tree

\item ”gain”: average gain of splits which use the feature

\item ”cover”: average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split
\end{itemize}

These model interpretation plots are not consistent, and conclusions about the contribution of each feature to the model are not reliable. In order to obtaint reliability in model interpretation, especially if we want to interpret the contribution of the features to single results , we need a method that is both consistent and accurate. SHAP plots ~\cite{DBLP:journals/corr/LundbergL17, lundberg2020local2global} are then \href{https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27}{the ideal choice}.

\item \textbf{SHAP model interpretation}

The implementation is available at {\tt SHAP\_error\_evaluation.py} and {\tt EDA\_features\_important.py}.
General SHAP model interpretation plots, such as the bar chart or the summary plot, allow to see wich of the features are contributing to the predictions and their ordering. We can observe in both plots (Figures \ref{fig:shapBar},\ref{fig:shapSummary}, that 4 out of the first seven features were generated during data preprocessing. In particular, the profile and frequency scores make important contributions to the model.

Summary plot (Figure \ref{fig:shapSummary}) also represents how the values of each feature contribute to the classification. We can see, for example, that high economic goals apper to be more frequently associated to failed projects. In fact, if we represent the distribution of 'usd\_goal' in successful or failed projects [Figure \ref{fig:goal}), we can see that succesful projects have economic goals that are centered in lower goal values.

We can also observe that higher values of the profile and frequency scores are associated to successful outcomes, see Figures \ref{fig:profile} and \ref{fig:freq}.

\begin{figure}
  \begin{subfigure}{1\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/shap_plots/bar_chart_mean_importance_xgbcl_wo_spot_test.pdf}
  \end{subfigure}
\caption{Mean importance chart}
\label{fig:shapBar}
\end{figure}

\begin{figure}
  \begin{subfigure}{1\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/shap_plots/summary_plot_xgbcl_wo_spot_test.pdf}
  \end{subfigure}
\caption{Summary plot}
\label{fig:shapSummary}
\end{figure}

\begin{figure}
  \begin{subfigure}{1\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/important_features_EDA/usd_goal/ylim_usd_goal_state.pdf}
  \end{subfigure}
\caption{Goal distribution in successful and failed projects}
\label{fig:goal}
\end{figure}

\begin{figure}
  \begin{subfigure}{1\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/important_features_EDA/profile/profile_state.pdf}
  \end{subfigure}
\caption{Profile score distribution in successful and failed projects}
\label{fig:profile}
\end{figure}

\begin{figure}
  \begin{subfigure}{1\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/important_features_EDA/frequency_score/frequency_score_state.pdf}
  \end{subfigure}
\caption{Frequency score distribution in successful and failed projects}
\label{fig:freq}
\end{figure}

\item \textbf{Prediction error evaluation using SHAP force plots}

The implementation is available at {\tt SHAP\_error\_evaluation.py}.

SHAP force plots are representations of feature contribution for individual projects. If we compute force plots for missclassfied projects, we can easily evaluate prediction errors and potentially increase model performance. In fact, the analyzed missclassified projects showed a common pattern (Figures \ref{fig:shapforce1}, \ref{fig:shapforce2}). The analyzed prediction errors were found to have low profile scores and frequency scores that corresponded to the predicted class. Therefore, if this diagnosis holds true after evaluation of a bigger representation of the prdiction errors, we might introduce new features to buffer the predictive power of both scores (see \ref{sec:conclu}).

\begin{figure}
  \begin{subfigure}{1\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/shap_plots/force/force_logit_miscla_xgbcl_wo_spot_test_10146_suc_pred_fail_ret.pdf}
    \caption{Successful predicted as failed}
\label{fig:shapforce1}
  \end{subfigure}
  \begin{subfigure}{1\linewidth}
    \centering\includegraphics[width=\linewidth]{../results/model/shap_plots/force/force_logit_miscla_xgbcl_wo_spot_test_121366_fail_pred_suc_ret.pdf}
    \caption{Failed predicted as successful}
\label{fig:shapforce2}
  \end{subfigure}
\caption{Force plots for misclassified projects}
\label{fig:shapforce}
\end{figure}

\end{itemize}

\section{Conclusions}
\label{sec:conclu}
\begin{itemize}

\item Creators should focalize their efforts in their project's profile, because is determinant for the success of their campaign. In future iterations of this project, the generation of more profile related features could help improve the model's performance. Information coded as urls in the JSON encoded variables could be valuable in this context. For example, processing the information contained in the images related to the project, could potentialy increase model accuracy.

\item Keywords were also valuable for the model performance, but might be the source of prediction errors. Other text mining methods could help to mitigate this effect, such as word embedding tecniques. Incorporation of information related to rewards (available at an url) could also be valuable. In addition, contextualizing keywords by combining google trends interest metrics, instagram and, twiter tags in months before and for the duration of a project's crowdfunding could also help to mitigate prediction errors. An evaluation of simultaneuos similar projects might also contribute to reduce errors.

\item Information about creators and their previous projects could also be retrieved and contribute to model performance. 

\end{itemize}

\section{Development tool}
\label{sec:devTools}

The following development tools were employed:
\begin{itemize}
\item Python
\item Git and GitHub
\item Visual Studio Code
\item iPython and Jupyter Notebooks
\item LaTeX
\item A Bash script allowing to reproduce the analysis is available at  {\tt kickstarter.sh}.
\end{itemize}

\bibliographystyle{unsrt} % Sorted references: [1], [2], ..
\bibliography{bibliography}

% test wordclouds
\begin{figure}
  \begin{subfigure}[a]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Art_successful.pdf}
    \caption{Successful Art}
    \label{fig:Art_s_test}
  \end{subfigure}
  \begin{subfigure}[a']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Art_failed.pdf}
    \caption{Failed Art}
    \label{fig:Art_f_test}
  \end{subfigure}
  \begin{subfigure}[b]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Comics_successful.pdf}
    \caption{Successful Comics}
    \label{fig:Comic_s_test}
  \end{subfigure} 
  \begin{subfigure}[b']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Comics_failed.pdf}
    \caption{Failed Comics}
    \label{fig:Comic_f_test}
  \end{subfigure}
  \begin{subfigure}[c]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Crafts_successful.pdf}
    \caption{Successful Crafts}
    \label{fig:Crafts_s_test}
  \end{subfigure}
    \begin{subfigure}[c']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Crafts_failed.pdf}
    \caption{Failed Crafts}
    \label{fig:Crafts_f_test}
  \end{subfigure}
    \begin{subfigure}[d]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Dance_successful.pdf}
    \caption{Successful Dance}
    \label{fig:Dance_s_test}
  \end{subfigure}
    \begin{subfigure}[d']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Dance_failed.pdf}
    \caption{Failed Dance}
    \label{fig:Dance_f_test}
   \end{subfigure}
    \begin{subfigure}[e]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Design_successful.pdf}
    \caption{Successful Design}
    \label{fig:Design_s_test}
  \end{subfigure} 
  \begin{subfigure}[e']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Design_failed.pdf}
    \caption{Failed Design}
    \label{fig:Design_f_test}
  \end{subfigure} 
    \begin{subfigure}[f]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Fashion_successful.pdf}
    \caption{Successful Fashion}
    \label{fig:Fashion_s_test}
  \end{subfigure} 
  \begin{subfigure}[f']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Fashion_failed.pdf}
    \caption{Failed Fashion}
    \label{fig:Fashion_f_test}
  \end{subfigure}
    \begin{subfigure}[g]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Film_Video_successful.pdf}
    \caption{Successful Film \& Video}
    \label{fig:Film_Video_s_test}
  \end{subfigure} 
  \begin{subfigure}[g']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Film_Video_failed.pdf}
    \caption{Failed Film \& Video}
    \label{fig:Film_Video_f_test}
  \end{subfigure}
    \begin{subfigure}[h]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Food_successful.pdf}
    \caption{Successful Food}
    \label{fig:Food_s_test}
  \end{subfigure} 
  \begin{subfigure}[h']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Food_failed.pdf}
    \caption{Failed Food}
    \label{fig:Food_f_test}
  \end{subfigure}
    \begin{subfigure}[i]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Games_successful.pdf}
    \caption{Successful Games}
    \label{fig:Games_s_test}
  \end{subfigure} 
  \begin{subfigure}[i']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Games_failed.pdf}
    \caption{Failed Games}
    \label{fig:Games_f_test}
  \end{subfigure}
  \begin{subfigure}[j]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Journalism_successful.pdf}
    \caption{Successful Journalism}
    \label{fig:Journalism_s_test}
  \end{subfigure} 
  \begin{subfigure}[j']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Journalism_failed.pdf}
    \caption{Failed Journalism}
    \label{fig:Journalism_f_test}
  \end{subfigure}
  \begin{subfigure}[k]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Music_successful.pdf}
    \caption{Successful Music}
    \label{fig:Music_s_test}
  \end{subfigure} 
  \begin{subfigure}[k']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Music_failed.pdf}
    \caption{Failed Music}
    \label{fig:Music_f_test}
  \end{subfigure}
  \begin{subfigure}[l]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Photography_successful.pdf}
    \caption{Successful Photography}
    \label{fig:Photography_s_test}
  \end{subfigure} 
  \begin{subfigure}[l']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Photography_failed.pdf}
    \caption{Failed Photography}
    \label{fig:Photography_f_test}
  \end{subfigure}
    \begin{subfigure}[m]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Publishing_successful.pdf}
    \caption{Successful Publishing}
    \label{fig:Publishing_s_test}
  \end{subfigure} 
  \begin{subfigure}[m']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Publishing_failed.pdf}
    \caption{Failed Publishing}
    \label{fig:Publishing_f_test}
  \end{subfigure}
    \begin{subfigure}[n]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Technology_successful.pdf}
    \caption{Successful Technology}
    \label{fig:Technology_s_test}
  \end{subfigure} 
  \begin{subfigure}[n']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Technology_failed.pdf}
    \caption{Failed Technology}
    \label{fig:Technology_f_test}
  \end{subfigure}
    \begin{subfigure}[o]{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Theater_successful.pdf}
    \caption{Successful Theater}
    \label{fig:Theater_s_test}
  \end{subfigure} 
  \begin{subfigure}[o']{0.2\linewidth}
    \centering\includegraphics[width=\linewidth]{../images/wordclouds/wc_test_Theater_failed.pdf}
    \caption{Failed Theater}
    \label{fig:Theater_f_test}
  \end{subfigure}

\caption{Wordclouds of successful and failed projects by principal category in the test set.}
\label{fig:wordclouds_test}
\end{figure}
\end{document}
