%\documentclass[a4paper]{article}

\documentclass{article}
\usepackage[a4paper]{geometry}

\usepackage[utf8]{inputenc}
\usepackage{import}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float} 
\usepackage{xcolor}
\usepackage{subfigure}
\usepackage{caption}
\usepackage{listings} % JSON listing
\usepackage{eurosym}
\usepackage{parskip} % Do not indent the beginning of each paragraph
\usepackage{comment}
\usepackage{booktabs}

%\renewcommand{\sectionname}{Lecture} % Redefine "chapter" to a different text

\newcommand{\ToDo}[1]{\textcolor{magenta}{\textbf{[ToDo]} \textbf{#1}}}
\newcommand{\gloria}[1]{\textcolor{blue}{[Gloria] {#1}}}


\title{Classification of Kickstarter's successful or failed projects based on data crawled by a scraper robot (Web Robots)}
\author{Gloria GONZÁLEZ CURTO}
\date{\today}

\begin{document}

\maketitle

\tableofcontents


\section{Summary}
\label{ref:sum}
The aim of this project is to classify and ultimately predict the success or failure of a kickstarter fund rising campaign based on data monthly crawled from the kickstarter.com web site by a scraper robot (webrobots.io).

\section{Problem definition and background}
{Kickstarter is an American corporation with a public benefit status launched in 2009 in the United States. The company initiated its international expansion in 2012. The kickstarter platform is open to backers from anywhere in the world and to creators from many countries \ref{subsec:EDA}.

Kickstarter maintains a global crowdfunding platform focused on creativity. Project creators choose a deadline and a minimum funding goal. If the goal is not met by the deadline, no funds are collected and the project is considered failed. Project backers usually receive rewards in exchange for their pledges.
Once a project collects enough pledges to bypass the goal before a deadline stablished by the creator, its state changes to successful. Only at that moment pledges are collected from backers. From that point onward, there is no guarantee for project delivery. Kickstarter advises backers to use their own judgment on supporting a project. They also warn project leaders that they could be liable for legal damages from backers for failure to deliver on promises.
 
On one hand, from a backer point of view, it would be interesting to have an analytic model to help with the decision of whether to pledge for a project or not. On the other hand, from the project creators, it would be interesting to have a guide to help them optimize their chances of success.

\section{Data description and pre-processing}
\label{sec:data_desc}

Raw data were collected from \url{https://webrobots.io/kickstarter-datasets/} in CSV format.  The downloaded data correspond to a crawl executed on February 13 2020. Raw data are composed by 57 CSV files with 3500 to 4000 rows each. The raw original variables are:
\begin{itemize}

    \item \textbf{backers\_count}: integer representing how many people supported the project (backers).
    \item \textbf{blurb}: textual description of the project (around 150-200 characters).
    \item \textbf{category}: JSON-encoded description on the kickstarter's fixed categories: id, name, slug, position, parent\_id, parent\_name, color, urls.

    \item \textbf{converted\_pledged\_amount}: pleged amount converted to USD.
    \item \textbf{country}: country initials (i.e: FR for France):
    \item \textbf{country\_displayable\_name}: country name.
    \item \textbf{created\_at}: date on Unix time format.
    \item \textbf{creator}: JSON-encoded description on project's creator. id, name, is\_registered, chosen\_currency, is\_superbacker, avatar, small, medium, url\_web, url\_api. 
    \item \textbf{currency}: currency acronym.
    \item \textbf{currency\_symbol}.
    \item \textbf{currency\_trailing\_code}: boolean variable.
    \item \textbf{current\_currency}: USD.
    \item \textbf{deadline}: crowdfunding deadline in Unix time established by the project's creator.
    \item \textbf{disable\_communication}: boolean variable.
    \item \textbf{friends}.
    \item \textbf{fx\_rate}: conversion rate.
    \item \textbf{goal}: fund rising goal in creator's fixed currency.
    \item \textbf{id}: project identifier.
    \item \textbf{is\_backing}.
    \item \textbf{is\_starrable}.
    \item \textbf{is\_starred}.
    \item \textbf{launched\_at}: launch date on Unix time format.
    \item \textbf{location}: JSON-encoded description of the project's creator location: id, name, slug, short\_name, displayable\_name, localized\_name, country, state, type, is\_root, expanded\_country, url\_web, url\_location, url\_api\_nearby\_projects.
    \item \textbf{name}: project's name.
    \item \textbf{permissions}.
    \item \textbf{photo}: JSON-encoded description on project's photos: key, full, ed, med, little, small, thumb, 1024x576, 1536x864.
    \item \textbf{pledged}: pledged amount in original country currency.
    \item \textbf{profile}: JSON-encoded description on project's profile: id, project\_id, state, state\_changed\_at, name, blurb, background\_color, text\_color, link\_background\_color, link\_text\_color, link\_text, link\_url, show\_feature\_image, background\_image\_opacity, should\_show\_feature\_image\_section, feature\_image\_attributes, image\_urls. 
    \item \textbf{slug}: small textual description with - as spacers.
    \item \textbf{source\_url}: url pointing to project's category site on kickstarter.
    \item \textbf{spotlight}: boolean representing if the project was featured on kickstarter web site.
    \item \textbf{staff\_pick}: boolean representing if the project was picked by kickstarter staff.
    \item \textbf{state}: project's state initialy composed by 5 categories (successful, failed, canceled, suspended and live.
    \item \textbf{state\_changed\_at}: date in Unix time format.
    \item \textbf{static\_usd\_rate}: static rate for currency conversion into USD.
    \item \textbf{urls}: JSON-encoded description containing urls: project, rewards.
    \item \textbf{usd\_pledged}.
    \item \textbf{usd\_type}: domestic.
    
\end{itemize} 

\subsection{Data pre-processing}
\label{subsec:data_prepro}
The following steps were followed to obtain a clean dataset for model training or EDA from raw data.
\begin{itemize}
    \item \textbf{Join CSV files}.
Raw data contain ambigous JSON-encoded fields that induce errors while parsing. To avoid those errors, ambigous JSON-encoded containing rows were filtered out while joining data into an unique file (\emph{pre\_join.py}).

    \item \textbf{Deserialize JSON-encoded columns}.
Columns and rows with missing information were dropped ("friends", "is\_backing", "is\_starred", "permissions") from raw data before JSON deserialization.
JSON-encoded columns (\emph{'category', 'creator', 'location', 'photo', 'profile', 'urls'}) were isolated from the rest of raw data and deserialized using a custom made function (\emph{deserialize\_in\_batch}) by means of the json\_normalize method from the pandas package and loads from the json package. The implementation can be accessed at \emph{pre\_decode\_JSON.py}).

    \item \textbf{Selection and pre-processing of deserialized variables}.
    The implementation of the following steps can be accessed at \emph{pre\_deserialized\_to\_one\_hot\_encoding.py}.
    \begin{itemize}
 
        \item Drop category\_slug because it contains partial redundant information with category\_name and category\_parent\_name.
        \item Fill category\_parent\_name missing values with their corresponding category\_name value.
        \item Exploration of location related variables: location\_localized\_name, location\_country, and location\_expanded\_country
	\begin{itemize}
            \item location\_localized\_name contains no missing values and 12977 unique values. Such a number of levels for a categorical variable might not be informative. Drop location\_localized\_name.
            \item location\_country and location\_expanded\_country are redundant. Drop location\_country because is less human readable.
	\end{itemize}
	\item Profile related variables. Codification of profile related variables into a binary code reflecting the involvement of the project creator in the generation of a profile.
	\begin{itemize}
            \item Re-codification of categorical binary variables:
	    profile\_state: "inactive"=0, "active"=1
            profile\_show\_feature\_image: False=0, True=1
            profile\_should\_show\_feature\_image\_section: False=0, True=1

            \item The rest of the profile related columns are going to be coded in a binary choice variable (missing value = 0= , variable contains a project creator provided value =1). The following columns were processed: 'profile\_name', 'profile\_blurb', 'profile\_background\_color', 'profile\_text\_color', 'profile\_link\_background\_color', 'profile\_link\_text\_color', 'profile\_link\_text', 'profile\_link\_url'.
        \end{itemize}
	\item One hot encoding of categorical variables while keeping the original columns for EDA, see section \ref{subsec:EDA}.
        The variables 'category\_name', 'category\_parent\_name', 'location\_expanded\_country' were codified using pd.get\_dummies and the argument drop\_first was set to True.
   \end{itemize}
   \item \textbf{Change date format and compute date derived variables}.
    The implementation can be accessed at \emph{pre\_date\_format\_and\_derived\_variables.py}.
    
    First, date and time coding variables ('created\_at', 'deadline', 'launched\_at', 'state\_changed\_at') in the raw data are in Unix time format and need to be transformed into datetime. The following variables where computed from the original ones: weekday columns for each date variable, month columns for each date variable, year columns for each date variable.
    
    'initial\_found\_rising\_duration' was computed as the difference in days between 'deadline' and 'launched\_at'.
    
    'found\_rising\_duration' was computed as the difference in days between 'state\_changed\_at' and 'launched\_at'.
    
    'project\_set\_up\_duration' was computed as the difference in days between 'launched\_at' and 'created\_at'.
    
    Date variables were prunned before model training to remove the 'state\_changed\_at' derived ones.
    
    \item \textbf{Currency derived variables pre-processing}.
    The implementation can be accessed at \emph{pre\_currency\_and\_other\_variables.py}.
    
    The following original variables were dropped due to redundancies or because they are not informative for the model: 'current\_currency', 'currency\_trailing\_code', 'fx\_rate', 'converted\_pledged\_amount', 'pledged', 'slug', 'usd\_type'.
    
    The variable 'usd\_goal' was computed by multiplication of the variables 'goal' 'and 'static\_usd\_rate' to able to compare the pledge goals for all the projects. 'goal' and 'static\_usd\_rate' were dropped after the generation of 'usd\_goal'.
    
    The variable 'is\_starrable' was also dropped, because of lack of relevance.
    
    The variables 'disable\_communication', 'spotlight' and 'staff\_pick' were recodified by replacing False=0 and True=1.
    
    The variable currency was dummified using get\_dummies (the original varaible was kept for visualization purpose).

    \item \textbf{State pre-processing (target variable)}.
    The implementation can be accessed at \emph{pre\_currency\_and\_other\_variables.py}.
    
    State is the target variable for classification. The raw variable is a categorical variable with 5 levels('successful', 'failed', 'live', 'canceled', 'suspended') and  no missing values.
    
    The variable 'state\_group' was generated by grouping the canceled and suspended projects into 'failed'. It contains then 3 distinct levels ('successful', 'failed' and 'live').
    
    The variable 'state\_code' was generated by dummification from 'state\_grouped' (failed = 0, successful=1, live=2). There are 114940 successful projects, 84311 failed projects and 6651 live projects.

   \item \textbf{Join the JSON decoded variables to the rest of the data}.
   The implementation can be accessed at \emph{pre\_join\_and\_drop\_live\_state.py}.
   
    \item \textbf{Drop rows corresponding to projects on a "live" state}.
    The implementation can be accessed at \emph{pre\_join\_and\_drop\_live\_state.py}.
    
    \item \textbf{Evaluate and drop duplicate rows}.
    The implementation can be accessed at \emph{pre\_duplicates.py}.
    21392 duplicate rows were detected and removed before further processing using pandas duplicated and drop\_duplicates functions.
    
    \item \textbf{Drop variables carrying low information}.
    Data dimensions at this processing stage was 177859 rows and 428 columns. In the following scripts I will evaluate the informative potential of different subsets of variables and drop the less informative ones in order to reduce the number of binary encoded columns and reduce the overfitting potential.
    
    \begin{itemize}
	\item Generation of the variable 'profile'.
        The implementation can be accessed at \emph{pre\_profile\_pruning.py}.
	
	'profile' was generated by addition of the following columns: 'profile\_ state', 'profile\_ name', 'profile\_ blurb', 'profile\_ background\_ color']+ df['profile\_ text\_ color', 'profile\_ link\_ background\_ color', 'profile\_ link\_ text\_ color', 'profile\_ link\_ text', 'profile\_ link\_ url', 'profile\_ show\_ feature\_ image', 'profile\_ should\_ show\_ feature\_ image\_ section'. It represents an score accounting for profile completeness. The variables were dropped after the score was computed.
        
	The number of columns was reduced to 418.
	
	\item Category name pruning.
	The implementation can be accessed at \emph{pre\_category\_name\_pruning.py }.
	
	'category\_name\_ori' encodes 159 subcategories of the 15 parent categories for kickstarter's projects. In order to evaluate the information content of each of these subcategory levels, plots and percentages of successful projects by secondary category were plotted to decide which one hot encoded columns under category\_name to drop. Only categories with a percentage of successful projects of at least 55\% (project success rate for the data is 53\%) that were informative within their parent categories were kept.
	After verification of plots and percentages, all comic, theater, dance secondary categories contain more successful tha failed projects (Figure1). As they are not more informative than their parent categories, those secondary categories were removed. 
	
	Figure1 (pdfs: nprojects\_ Comics\_ state.pdf, nprojects\_ Dance\_ state.pdf, nprojects\_ Theater\_ state.pdf)
	
	The following subcategories were kept: 'Anthologies', 'Letterpress', "Children's Books", 'Art Books', 'Publishing', 'Literary Spaces', 'Fiction', 'Nonfiction', 'Playing Cards', 'Video Games', 'Tabletop Games', 'Games', 'Puzzles', 'Journalism', 'Public Art', 'Illustration', 'Painting', 'Art', 'Social Practice', 'Shorts', 'Narrative Film', 'Documentary', 'Webseries', 'Film \& Video', 'Crafts', 'Photography', 'Product Design', 'Typography', 'Design', 'Classical Music', 'Country \& Folk', 'Indie Rock', 'Pop', 'Music', 'Jazz', 'Comedy', 'Rock', 'Chiptune', 'Apparel', 'Accessories', 'Fashion', 'Gadgets', 'Hardware', 'Technology'.
	
	The number of columns was reduced to 304.
	
        \item Country pruning.
        The implementation can be accessed at \emph{pre\_country\_pruning.py}.  
	
	Projects in the dataset come from 198 countries. In order to reduce the number of variables, I performed plots and percentages of successful projects by country and counts of projects by country. Only countries with more than 55\% of successful projects and more than 50 projects were kept (United Kingdom, Hong Kong, Japan, Singapore, China, Poland, Israel, Taiwan, Czech Republic, Greece, Indonesia, Argentina, Kenya, Iceland, Ghana, Portugal, Slovenia, Finland).
	
	The number of columns was reduced to 125.
	
	\item  Currency pruning.
	The implementation can be accessed at \emph{pre\_currency\_pruning.py }.
	
	Currency encodes 14 different levels. Plots and percentages of succesfull projects were performed for 'currency\_orig'. Currencies with more than 55\% of successful projects were retained (GBP, HKD, SGD, JPY) and compared to the retained countries. All currency levels were dropped because the selected currencies correspond to countries that were already selected and the information was redundant.
	
	The number of columns was reduced to 112.
        
    \end{itemize}
    \item \textbf{Drop rows with text in other languages than English}.
    The implementation can be accessed at \emph{pre\_words.py}.
    
    The variables blurb and name contain brief descriptions of the project. In order to obtain valuable information for the model, these text variables are going to be processed to obtain keywords and compute a score due to their presence in the text. Project\'s texts are writen in several languages. Several attempts to translate all text into English were performed using langdetect package. Unfortunately, the quantity of characters to translate was bigger than the established API limits. The estimation of the fees for the translation using the google translate API was 1200~\euro.
    
    English rows were detected and isolated using a custom made function. Briefly, for each row (project), blurb and name texts were tokenized and then compared to an extense list of 370101 English words \url{(https://github.com/dwyl/english-words/blob/master/words_dictionary.json)}. An score was computed considering tokens with more than 3 characters and the lenght of the text. Rows with an score lower than 0.8 were considered not written in English and discarded (55269 rows).
    
    Data dimension after filtering was 122590 rows and 112 columns.
    
    To be able to extract category keywords associated to successful or failed projects, data were splitted in training and test sets at this point.
    
    \item \textbf{Split train and test}.
    The implementation can be accessed at \emph{pre\_split\_train\_test.py }.
    
    Before splitting the data in train and test sets, I visualized the projects by year ("state\_changed\_at") and state (FIG). The implementation can be accessed at \emph{pre\_year\_EDA.py} and \emph{pre\_data\_prep\_temporal\_and\_classification.py}. The target variable ( state) shows no clear correlation with time, as we can observe in the plots (FIG) and our data are not evenly spaced. Therefore, after close examination of our data, I decided to implement a random split of train and test sets using the train\_test\_split functiikit learn model\_selection package, a train size of 73 \% a 37 as random seed. Before spliting, 15 columns were dropped, containing information not available prior to the project's state change. Dimensions of training set are 89490 rows and 97 columns, and dimensions of test set are 33100 rows and 97 columns. 
    FIG 

	
    \item \textbf{Generation of the frequency score feature}.
    The implementation can be accessed at \emph{pre\_text\_mining.py}.
    Title and descriptions of the project were processed to obtain an score based on word frequencies in succesful and failed projects by principal category ('category\_parent\_name', 15 categories). Word  frequencies were computed using both 'blurb' and 'name' variables on the training set. Punctuation signs were replaced by blank spaces, words were tokenized (word\_tokenize from nltk package), stop words were filtered (stopwords from nltk), and stemmed (PorterStemmer from nltk). Word frequency was computed using FreqDist (nltk package). The frequency score was computed for every project by adding the successful project frequency of every word present in its 'blurb' and 'name' variables and substracting the failed project frequencies. The score was normalized by text lenght.  

\end{itemize}

\subsection{Exploratory data analysis}
\label{subsec:EDA}
\begin{itemize}

\item \textbf{Profile reports} generated multiple times during data pre-processing using the pandas profile package (\url{https://pypi.org/project/pandas-profiling/}) to help with data visualization. For clarity purposes, only raw, train and test data htlm profiling reports can be found in annex files.
The implementation can be accessed at \emph{EDA\_profiling.py} and \emph{EDA\_tt\_profile.py }.

Pandas profiling generates an statistical description of the data and each variable, computes interaction and correlation plots and warns about potential sources of problems such as high correlation, duplicates, high cardinality for categorical variables and skewed or zero containing variables.
These reports were extremely useful to quickly identify duplicated rows, variables with high numbers of missing values and also non numerical variables present in the train and test sets that are incompatible with XGBoost modeling.

\item \textbf{Word clouds} were generated from the train and test sets by principal category and successful or failed projects using WordCloud. 
The implementation can be accessed at \emph{pre\_text\_mining.py}.

Word clouds were representative of each category and sometimes were helpful to identify subcategories with high rates of failed projects (such as food trucks within Food, see fig). The majority of the most frequent 200 words were shared by successful and failed projects within categories but they also always present especific more frequent words.
If we take as an example the Comic word clouds, we can see that both successful and failed projects contain graphic novel, adventure and comic with aproximatively the same frequency, but horror, fantasy and manga, are overrepresented in the failed projects. At the same time, anthologies, sci fi and girl terms are overrepresented in the succesful projects, suggesting possible differences in the succesful rate of different comic genres (Figure).
Other words were consistenly found associated to successful projects across categories. This is the case, for example, of enamel pin and hard enamel that were associated to succesful projects from the crafts and fashion categories, and absent from the 200 most frequent words (FIG).

For clarity purpose, test set word clouds can be found as an annex (annex1).

Word clouds were useful to understand project dinamics within categories and to evaluate the pertinency of the frecuency score feature. As we can identify trends in the word clouds associated to the target variable, the frequency score could potentialy help increase the evaluation metrics of our model.

FIG only with train

The rest of data visualization was performed as a guide during data pre-processing (ref figures) or the interpretation of the model (ref figures).

\end{itemize}


\section{Data modeling}
\label{sec:model}
\begin{itemize}
\item \textbf{Data processing prior to XGBoost training}
The implementation can be accessed at \emph{pre\_data\_model\_train.py} and \emph{pre\_data\_model\_test.py}.
First, the target variable in numeric format ('state\_code') was isolated from the features and written to a file.
Second, non numeric features were identified thanks to the pandas-profiling report and removed. Features were written to a file.

\item \textbf{Methodological choices}
The following packages were used for data modeling, evaluation and interpretation:
\begin{itemize}
	\item xgboost.
	Extreme Gradient Boosting classifier~\cite{DBLP:journals/corr/ChenG16} was chosen due to its performance, escalabilty and built-in model interpretation capabilities.
The tree based booster was prefered because of the intuitive interpretation of tree models.
	\item Scikit learn matrics: classification\_report, confusion\_matrix~\cite{pedregosa2011scikit}.
	\item bayesian-optimization~\cite{nogueiraBayOpt}
	Bayesian optimization with cross validation was prefered as hyperparameter tuning method over a grid or a random search because it has been shown to obtain better results in fewer evaluations~\cite{snoek2012practical, NIPS2011_4443}.

	\item SHAP
	SHAP ~\cite{DBLP:journals/corr/LundbergL17, lundberg2020local2global} was prefered over other model interpretation methods due to its mathematical strenght.
\end{itemize}

\item \textbf{Model training, first iteration}

The implementation can be accessed at \emph{train\_first\_model\_wo\_ts.py}.

As a first attempt to model the data, a XGboost tree bosster classifier was initialized with default parameter and fitted to the training data. Evaluation metrics for predictions using both the training and test set were consistently found to show a score of 1, see tables \ref{table:model1_tr_cr}, \ref{table:model1_te_cr}.

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &  recall &  f1-score &  support \\
\midrule
0            &        1.0 &     1.0 &       1.0 &  41522.0 \\
1            &        1.0 &     1.0 &       1.0 &  47968.0 \\
accuracy     &        1.0 &     1.0 &       1.0 &      1.0 \\
macro avg    &        1.0 &     1.0 &       1.0 &  89490.0 \\
weighted avg &        1.0 &     1.0 &       1.0 &  89490.0 \\
\bottomrule
\end{tabular}
\caption{Classification report for training set}
\label{table:model1_tr_cr}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &  recall &  f1-score &  support \\
\midrule
0            &        1.0 &     1.0 &       1.0 &  15423.0 \\
1            &        1.0 &     1.0 &       1.0 &  17677.0 \\
accuracy     &        1.0 &     1.0 &       1.0 &      1.0 \\
macro avg    &        1.0 &     1.0 &       1.0 &  33100.0 \\
weighted avg &        1.0 &     1.0 &       1.0 &  33100.0 \\
\bottomrule
\end{tabular}
\caption{Classification report for test set}
\label{table:model1_te_cr}
\end{table}

In order to diagnose the reason behind such a perfect metrics, training set label randomization was performed using shuffle from scikit learn utils package. A new model was fitted with the shuffled labels and predictions for both the training and test sets were evaluated. As shown in tables \ref{table:model1_tr_sh_cr}, \ref{table:model1_te_sh_cr} and in their corresponding confusion matrices  \ref{table:model1_tr_sh_cm}, \ref{table:model1_te_sh_cm}, prediction was randomized after shuffling the labels, as proben by an accuracy of  50\% in predictions for the test set. This results suggest that the perfect scores are not due to overfitting.
 
\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.726857 &  0.325442 &  0.449587 &  41522.000000 \\
1            &   0.604945 &  0.894138 &  0.721647 &  47968.000000 \\
accuracy     &   0.630272 &  0.630272 &  0.630272 &      0.630272 \\
macro avg    &   0.665901 &  0.609790 &  0.585617 &  89490.000000 \\
weighted avg &   0.661510 &  0.630272 &  0.595415 &  89490.000000 \\
\bottomrule
\end{tabular}
\caption{Classification report for predictions of the train set using the model trained with shuffled labels}
\label{table:model1_tr_sh_cr}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrr}
\toprule
{} &      0 &      1 \\
\midrule
0 &  13513 &  28009 \\
1 &   5078 &  42890 \\
\bottomrule
\end{tabular}

\caption{Confusion matrix for predictions of the train set using the model trained with shuffled labels}
\label{table:model1_tr_sh_cm}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.419684 &  0.184141 &  0.255971 &  15423.000000 \\
1            &   0.522159 &  0.777847 &  0.624858 &  17677.000000 \\
accuracy     &   0.501208 &  0.501208 &  0.501208 &      0.501208 \\
macro avg    &   0.470921 &  0.480994 &  0.440415 &  33100.000000 \\
weighted avg &   0.474410 &  0.501208 &  0.452975 &  33100.000000 \\
\bottomrule
\end{tabular}
\caption{Classification report for predictions of the test set using the model trained with shuffled labels}
\label{table:model1_te_sh_cr}
\end{table}
 
\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.419684 &  0.184141 &  0.255971 &  15423.000000 \\
1            &   0.522159 &  0.777847 &  0.624858 &  17677.000000 \\
accuracy     &   0.501208 &  0.501208 &  0.501208 &      0.501208 \\
macro avg    &   0.470921 &  0.480994 &  0.440415 &  33100.000000 \\
weighted avg &   0.474410 &  0.501208 &  0.452975 &  33100.000000 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for predictions of the test set using the model trained with shuffled labels}
\label{table:model1_te_sh_cm}
\end{table}

Tree and importance plots for model interpretation implemented in the XGBoost package solved the mistery. Interpretation plots showed that the decision tree had a unique branch and the predictions were solely based in the value of the feature 'spotlight', that was identified as highly correlated with the target variable by the pandas-profiling reports. FIG

After a search in the kickstater's website, I found a more exact difinition for 'spotlight' that the one provided with the data. Spotlight is a place for the projects on Kickstarter were the creators can communicate their project's progress, after they’ve been successfully funded.

I removed 'spotlight' from the features files and proceed to train a new model

\item \textbf{Model training, second iteration}

The implementation can be accessed at \emph{train\_wo\_spotlight.py}.

A bayesian Optimization function for xgboost was defined with the following parameters:
\begin{itemize}
	\item 'objective': 'binary:logistic'. Represents the learning task and the corresponding learning objective for a binary classification.
	\item 'max\_depth': int(max\_depth). Maximum depth of a tree. Higher values make the model more complex and more likely to overfit. Range: $(0, \infty)$.
	\item 'gamma': gamma. Minimum loss reduction required to partition a leaf node of the tree. Higher values make the algorithm more conservative. Range: $(0, \infty)$.
	\item'learning\_rate':learning\_rate. Step size shrinkage used in boosting iterations. Higher values make the process more conservative. Range: (0,1).
	\item 'subsample': subsample. Subsample ratio of the training instancesprior to growing trees. Subsampling will occur once in every boosting iteration and prevents overfitting. Range: (0,1).
	\item 'eval\_metric': 'auc'. AUC measures the quality of the model's predictions irrespective of what classification threshold is chosen and their absolute values. For our classification task, we don't need well calibrated outputs and the cost of classification errors in our case is not critical. Moreover, a paralel hyperparameter optimization was evaluated using 'error' as metric with similar results. Range: (0,1).

\end{itemize}

Cross validation was performed in 5 folds and 500 iterations, with and early stop at 100 rounds.
Hyperparameter search was evaluated using "test-auc-mean" as metric.

The bayesian hyperparameter space was designed in a conservative way, to avoid overfitting of the data.
\begin{itemize}
	\item 'max\_depth': (3, 8).  Default value is 6. 
	\item 'gamma': (0, 5). Default value is 0.
	\item 'learning\_rate':(0, 1). Default value is 0.3.
	\item 'subsample':(0, 1). Default value is 1.
\end{itemize}

Bayesian optimization was performed for 5 iterations with 8 steps of random exploration and an acquisition function of expected improvement ('ie'). Best parameters were optimized to \{'gamma': 3.262977074427203, 'learning\_rate': 0.08113939174319618, 'max\_depth': 7.84163489438336, 'subsample': 0.9915277034216099\}.

The following tables contain evaluation metrics obtained by training the model with the best hyperparameters defined by bayesian optimization:

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.833378 &  0.896802 &  0.863927 &  41522.000000 \\
1            &   0.904370 &  0.844792 &  0.873566 &  47968.000000 \\
accuracy     &   0.868924 &  0.868924 &  0.868924 &      0.868924 \\
macro avg    &   0.868874 &  0.870797 &  0.868747 &  89490.000000 \\
weighted avg &   0.871431 &  0.868924 &  0.869094 &  89490.000000 \\
\bottomrule
\end{tabular}
\caption{Classification report for predictions of the train set using bayesian optimization hyperparameters}
\label{table:model_tr_bo_cr}
\end{table}

\begin{table}[h!]
\centering

\begin{tabular}{lrr}
\toprule
{} &      0 &      1 \\
\midrule
0 &  37237 &   4285 \\
1 &   7445 &  40523 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for predictions of the train set using bayesian optimization hyperparameters}
\label{table:model_tr_bo_cm}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &      support \\
\midrule
0            &   0.829042 &  0.885107 &  0.856157 &  15423.00000 \\
1            &   0.893471 &  0.840754 &  0.866311 &  17677.00000 \\
accuracy     &   0.861420 &  0.861420 &  0.861420 &      0.86142 \\
macro avg    &   0.861256 &  0.862930 &  0.861234 &  33100.00000 \\
weighted avg &   0.863450 &  0.861420 &  0.861580 &  33100.00000 \\
\bottomrule
\end{tabular}
\caption{Classification report for predictions of the test set using bayesian optimization hyperparameters}
\label{table:model_te_bo_cr}
\end{table}
 
\begin{table}[h!]
\centering
\begin{tabular}{lrr}
\toprule
{} &      0 &      1 \\
\midrule
0 &  13651 &   1772 \\
1 &   2815 &  14862 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for predictions of the test set using bayesian optimization hyperparameters}
\label{table:model_te_bo_cm}
\end{table}

If we compare these classification reports, in tables \ref{table:model_tr_bo_cr}, \ref{table:model_te_bo_cr,} to the ones of a XGBoost classifier with default parameters, in tables, \ref{table:model_tr_cr}, \ref{table:model_te_cr}, we can observe similar metrics. However, the confusion matrices for the default hyperparameters model, see tables \ref{table:model_tr_cm}, \ref{table:model_te_cm}, were slighly better than for the bayesian optimization model, see tables \ref{table:model_tr_bo_cm}, \ref{table:model_te_bo_cm}. Therefore, I decided to save the default parameters model to a file for further exploitation.  The following step was model interpretation.

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.835575 &  0.906652 &  0.869664 &  41522.000000 \\
1            &   0.912773 &  0.845564 &  0.877884 &  47968.000000 \\
accuracy     &   0.873908 &  0.873908 &  0.873908 &      0.873908 \\
macro avg    &   0.874174 &  0.876108 &  0.873774 &  89490.000000 \\
weighted avg &   0.876955 &  0.873908 &  0.874070 &  89490.000000 \\
\bottomrule
\end{tabular}

\caption{Classification report for predictions of the train set using default hyperparameters}
\label{table:model_tr_cr}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrr}
\toprule
{} &      0 &      1 \\
\midrule
0 &  37646 &   3876 \\
1 &   7408 &  40560 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for predictions of the train set using default hyperparameters}
\label{table:model_tr_cm}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{lrrrr}
\toprule
{} &  precision &    recall &  f1-score &       support \\
\midrule
0            &   0.828747 &  0.892369 &  0.859382 &  15423.000000 \\
1            &   0.899351 &  0.839113 &  0.868188 &  17677.000000 \\
accuracy     &   0.863927 &  0.863927 &  0.863927 &      0.863927 \\
macro avg    &   0.864049 &  0.865741 &  0.863785 &  33100.000000 \\
weighted avg &   0.866453 &  0.863927 &  0.864085 &  33100.000000 \\
\bottomrule
\end{tabular}
\caption{Classification report for predictions of the test set using default hyperparameters}
\label{table:model_te_cr}
\end{table}
 
\begin{table}[h!]
\centering
\begin{tabular}{lrr}
\toprule
{} &      0 &      1 \\
\midrule
0 &  13763 &   1660 \\
1 &   2844 &  14833 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for predictions of the test set using default hyperparameters}
\label{table:model_te_cm}
\end{table}

\end{itemize}

\section{Model interpretation}
\label{sec:interp}

\begin{itemize}
\item \textbf{XGBoost built-in model interpretation}

The implementation can be accessed at~\emph{train\_wo\_spotlight.py}.

\item \textbf{SHAP model interpretation}

The implementation can be accessed at~\emph{SHAP\_error\_evaluation.py}

\item \textbf{Prediction error evaluation using SHAP force plots}

The implementation can be accessed at~\emph{SHAP\_error\_evaluation.py}

\item \textbf{Data visualization to support model interpretation}

The implementation can be accessed at~\emph{EDA\_features\_important.py}


\end{itemize}
\section{Conclusions}
\label{sec:conclu}

\begin{comment}
\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\linewidth]{RRPR_2016/images/architecture/architecture.pdf}
\caption{IPOL as a modular system.} 
\label{fig:architecture}
\end{figure}
\end{comment}

\bibliographystyle{unsrt} % Sorted references: [1], [2], ..
\bibliography{bibliography}

\end{document}
